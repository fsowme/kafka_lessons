- Этапы оптимизации кластера:
    1. Определить, что нужно оптимизировать - анализ текущей производительности и выявление узких мест (оценить
        нагрузку, задержки, пропускная способность и др.)
    2. Сконфигурировать кластер и клиентов - после определения мест для оптимизации нужно изменить конфигурацию как
        кластера, так и клиентов (размеры буферов, количество партиций, уровень репликации и др.)
    3. Бенчмаркинг и мониторинг - первое показывает влияние изменений на производительность системы, второе помогает
        отслеживать в реальном времени метрики и вовремя реагировать на проблемы

- Для определения, что нужно оптимизировать, вначале надо определить показатели производительности, которые необходимо
    достичь, ключевых четыре:
    - Пропускная способность
    - Задержка
    - Надежность
    - Доступность

- Одновременно достичь всех показателей невозможно, нужно искать компромисс

- Для определения приоритетных метрик для кластера нужно проанализировать варианты использования приложения и
    бизнес-требования, особенно важные для сценария использования. Обычно решение принимается коллективно - происходит
    обсуждение с командой, сосредоточенное на исходных сценариях использования и их приоритетах

- После выбора целевую метрику для оптимизации, нужно настроить параметры конфигурации для достижения этой цели. Цели:
    - Высокая пропускная способность - нужно обеспечить эффективную передачу данных от продюсеров брокерам, от брокеров
        консьюмерам. Иногда скорость достигает миллионов записей в секунду, Kafka позволяет записывать большие объемы
        данных значительно быстрее, чем в традиционных БД или хранилищах ключ-значение
    - Минимум задержки - нужно минимизировать время на передачу сообщений между продюсерами, брокерами и консьюмерами,
        часто это актуально для чатов и интерактивных веб-сайтов, где ожидается мгновенный отклик
    - Высокая надежность - нобходимо гарантировать, что зафиксированные сообщения не будут потеряны. Например, важно
        для конвейеров микросервисов с потоковой передачей событий, где Kafka - это хранилище событий. Еще это важно
        при интеграции между источниками потоковой передачи и постоянными хранилищами (например, AWS S3)
    - Высокая доступность - нужно минимизировать время простоя в случае сбоев

- Оптимизация пропускной способности - сообщения в разных партициях одного топика записываются и читаются одновременно,
    так реализуется параллельная обработка и распределяется нагрузка. Если увеличить количество партиций, обычно расчет
    пропускная способность и снижается риск появления "горячих точек". Но если слишком много партиций, могут быть
    негативные последствия

- Для повышения параллелизма необходимо расчитать нужное количество партиций для каждого топика Kafka. Для этого нужно
    на основе желаемой пропускной способности оценить количество консьюмеров и продюсеров на партицию. Например, если
    цель - скорость чтения 1 ГБ/с, а консьюмер может обрабатывать только 50 МБ/с, то нужно минимум 20 партиций и 20
    консьюмеров в группе. Аналогично для продюсеров, если один продюсер может писать со скоростью 100 МБ/с, то нужно 10
    партиций

- Простая формула расчета количества партиций:
    - примерное_количество_партиций = max(количество_косньюмеров, количество_продюсеров)

- Количество продюсеров/консьюмеров расчитывается на основе общей ожидаемой пропускной способности:
    - колчество_продюсеров = max_speed_per_partition / all_system_speed
    - количество_косньюмеров = max_speed_per_partition / all_system_speed
        - max_speed_per_partition - максимальная пропускная способность одного продюсера/консьюмера для одной партиции
        - all_system_speed - общая ожидаемая пропускная способность

- По формулам выше можно получить хорошую отправную точку, тоесть приблизительно расчитать нужное количество партиций
    на старте. Помимо этого нужно учитывать такие моменты:
    - Перераспределять партиции затратно, поэтому лучше взять с запасом
    - Изменить количество партиций, основанных на ключах, сложно, потребуется ручное копирование
    - Уменьшить количество партиций нельзя. Если это нужно сделать, надо создать новый топик и скопировать в него
        данные
    - При использовании кластера с ZooKeeper метаданные партиций хранятся на нем в виде znodes. Большое количество
        партиций влияет на ZooKeeper и ресурсы клиентов
    - Чем больше партиций, тем больше открытых файловых дескрипторов. Следует установить максимум их количества. При
        этом важно иметь определённый минимум файловых дескрипторов, чтобы брокер эффективно отслеживал файлы сегментов
        журналов. Расчитать разумное количество дескрипторов поможет формула:
        - количество_дескрипторов = количество_партиций * (размер_партиции/размер_сегмента)
        - + брокеру нужны дополнительные дескрипторы для связи через сетевые сокеты с внешними сторонами (клиентами,
            другими брокерами, ZooKeeper, Kerberos)

- Дополнительные аспекты при оптимизации пропускной способности:
    - Размер пакета - ключевой аспект, использование пакетной обработки продюсерами. Продюсеры группируют сообщения в
        пакет => снижается количество запросов к брокерам и нагрузка на CPU. Эффективно использовать пакетную обработке
        помогают параметры:
        - batch.size - максимальный размер пакета сообщений
        - linger.ms - время ожидания продюсером перед отправкой пакета, позволяет пакету наполниться сообщениями и
            достичь заданного размера
        Увеличивая оба параметра увеличивается размер пакета => оптимизируем передачу, использование сетевых ресурсов
        НО необходимо следить за задержкой, тк увеличивается время отклика, важно не переборщить
    - Сжатие - позволяет передавать больше данных в меньшем объёме. Параметр отвечающий за сжатие "compression.type",
        его значения: lz4, snappy, zstd, gzip. Обычно рекомендуется lz4 и не рекомендуется gzip, тк он может сильно
        повлиять на CPU. Сжатие применяется к пакетам данных и его эффективность зависит от заполненности пакетов,
        заполненные пакеты обеспечивают лучший коэффициент сжатия. Если кодек сжатия топика совподает с кодеком
        продюсера, то брокер записывает сжатый пакет непосредственно в лог => не тратится время на повторное сжатие и
        производительность растёт
    - Время ожидания ответов - при отправке сообщение оно отправляется в лидера партиции, продюсер ожидает подтверждение
        фиксации от брокера, прежде чем отправить следующее. Встроенная автоматическая проверка гарантирует, что
        консьюмеры не могут читать незафиксированные сообщения. Время ответа брокера влияет на пропускную способность
        продюсера, оптимизировать процесс помогает параметр "acks" - количество подтверждений от реплик партиций, при
        "acks=1" брокер записывает сообщение в локальный журнал и отправляет подтверждение продюсеру, но
        отказоустойчивость падает, тк в случае падения мастера до синхронизации сообщения с репликами сообщение
        потеряется
    - Лимиты памяти - продюсеры автоматически выделяют память для хранения неотправленных сообщений, при достижении
        лимита продюсер блокирует дополнительные отправки до освобождения памяти и истечении времени, заданного
        параметром "max.blocks.ms". Объём памяти задает параметр "buffer.memory". При большом количестве партиций
        следует увеличить "buffer.memory", учитывая размер сообщений и время ожидания, так поддержится конвейерная
        обработка и оптимизируется пропускная способность брокеров
    - Объём данные при каждой выборке - чем больше этот объём, тем меньше запросов к брокеру => снижается нагрузка на
        CPU, повышается пропускная способность, НО увеличивается задержка, тк брокер ждёт пока в запросе не наберется
        нужное количество данных или не истечет заданное время, параметр за время - "fetch.max.wait.ms", параметр за
        объём "fetch.min.bytes"
    - Другие параметры - рекомендуется использовать группы консьюмеров с несколькими экземплярами, чтобы распараллелить
        обработку и сбалансировать нагрузку между партициями. Максимамальное количество консьюмеро в группе
        ограничивается количеством партиций в топике Так же важно настроить JVM для минимизации паузы во время сборки
        мусора (GC), тк длительные паузы понижают пропускную способность и могут привест к сбоям брокера

- Обработка больших сообщений - можно настроить Kafka на работу с большими сообщениями, изменяя параметры конфигурации
    брокера и консьюмера, касающиеся размера сообщений. Однако перед этим стоит рассмотреть другие варианты:
    - Сжать сообщения - например, исходные текстовые сообщения (например, XML) хорошо сжимаются ("compression.type")
    - Использовать общее хранилище - например, NAS, HDFS, S3. В большинстве случаем намного эффективнее размещать файлы
        в общем хранилище, а через Kafka отправлять сообщения с указанием места расположения файлоа
    - Разделение сообщений - можно разделить сообщения на сегменты, например размером в 1 KB, с помощью продюсера, потом
        отправить эти сегменты послодовательно, далее получить в такой же последовательности на консьюмере и
        восстановить исходное сообщение. Чтобы сегменты отправлялись в одну и ту же партицию можно использовать ключи

- Если варианты выше не подходят, то можно настроить следующие параметры:
    - Для продюсера
        - message.max.bytes - максимальный размер сообщения принимаемый брокером (1 000 000)
        - log.segment.bytes - размер файла данных Kafka, должен быть больше любого сообщения (1 073 741 824)
        - replica.fetch.max.bytes - максимальный размер сообщения, который брокер может реплицировать, значение должно
            превышать размер самого большого сообщения, иначе не все сообщения будут реплицироваться (1 048 576)
    - Для консьюмера
        - max.partition.fetch.bytes - максимальное количество данных на партицию, которое брокер вернёт (1 048 576)
        - fetch.max.bytes - максимальное количество данных, которое сервер может вернуть для выборки (1 073 741 824)

- Консьюмер моджет обработать партию сообщений, размер которой больше чем max.partition.fetch.bytes или fetch.max.bytes,
    но такая партия может отправиться отдельно, что снизит производительность

- Оценка размера кластера - наиболее точный способ это использование "kafka-producer-perf-test" и
    "kafka-consumer-pref-test" на целевом оборудовании для симуляции нагрузки

- Без симуляции оценить размер поможет формула
    объём_диска = ожидаемая_скорость_поступления_данных * требуемый_период_хранения

    - Пусть в системе
        - W - скорость записи данных в MB/с
        - R - фактор репликации
        - C - количество групп консьюмеров (число читателей для каждой записи)

        - Объём записи - каждая реплика записывает каждое сообщение => объём_записи = W * R
        - Объём чтения - данные читаются консьюмерами и репликами в процессе репликации. Каждый консьюмер читает каждую
            запись => объем_чтения_консьюмеров = C * W. Каждая реплика, кроме главной, тоже читает каждую запись =>
            объём_чтения_реплик = (R - 1) * W. А общий_объём_чтения = (R + C - 1) * W
        - Эффект кэширования - чтение может кэшироваться, в этом случае ввод-вывод на диске не происходит. Пусть кластер
            имеет M MB памяти, тогда скорость записи W MB/s позволяет кэшировать записи на время M / (W * R) секунд.
            Напрмер 32 ГБ свободной памяти позволяет хранить в кеше прмерно последние 10 минут данных при скорости
            записи 50 MB/s
        - Эффект выхода из кэша - читатели могут выйти из кэша по разным причинам, например из-за медленного консьюмера
            или сбоя сервера, который восстанавливается и нуждается в синхронизации. Смоделируем! Пусть количество
            отстающих от кэша читателей равно L. Пессимистичное предположение - все консьюмеры отстали, тогда
            L = R + C - 1. Более реалистичный пример - в один момент времени отстаёт не больше 2 консьюмеров

    - Тогда получаем такие требования к вводу-выводу кластера
        - Пропускная способность диска (чтение и запись) - W * R + L * W
        - Сетевая пропускная способность чтения (R + C - 1) * W
        - Сетевая пропускная сопосбность записи W * R

- Определение количества серверов
    - Один сервер обеспечивает определенную пропускную способность диска и сети. Например, сетевой адаптер 1 Gb/s с
        полудуплексным режимом обеспечит 125 MB/s на чтение и 125 MB/s на запись, а 6 дисков SATA 7200 обеспечит
        300 MB/s на чтение и запись. Зная общие требования и возможности одного сервера, посчитаем количество серверов

        - минимальное_число_серверов = общее_количество_нужных_ресурсов / ресурсы_одного_сервера

    - В формуле мы предполагаем, что машина работает на максимальной мощности, нет накладных расходов на протоколы, а
        нагрузка и данные распределены равномерно по серверам. Такого не бывает, чтобы учесть все накладные расходы,
        рекомендуется иметь как минимум в 2 раза больше этой идеально можности. Так обеспечится достаточная ёмкость.
        Получаем
            - достаточное_число_серверов = 2 * (общее_количество_нужных_ресурсов / ресурсы_одного_сервера)
