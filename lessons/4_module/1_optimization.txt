- Этапы оптимизации кластера:
    1. Определить, что нужно оптимизировать - анализ текущей производительности и выявление узких мест (оценить
        нагрузку, задержки, пропускная способность и др.)
    2. Сконфигурировать кластер и клиентов - после определения мест для оптимизации нужно изменить конфигурацию как
        кластера, так и клиентов (размеры буферов, количество партиций, уровень репликации и др.)
    3. Бенчмаркинг и мониторинг - первое показывает влияние изменений на производительность системы, второе помогает
        отслеживать в реальном времени метрики и вовремя реагировать на проблемы

- Для определения, что нужно оптимизировать, вначале надо определить показатели производительности, которые необходимо
    достичь, ключевых четыре:
    - Пропускная способность
    - Задержка
    - Надежность
    - Доступность

- Одновременно достичь всех показателей невозможно, нужно искать компромисс

- Для определения приоритетных метрик для кластера нужно проанализировать варианты использования приложения и
    бизнес-требования, особенно важные для сценария использования. Обычно решение принимается коллективно - происходит
    обсуждение с командой, сосредоточенное на исходных сценариях использования и их приоритетах

- После выбора целевую метрику для оптимизации, нужно настроить параметры конфигурации для достижения этой цели. Цели:
    - Высокая пропускная способность - нужно обеспечить эффективную передачу данных от продюсеров брокерам, от брокеров
        консьюмерам. Иногда скорость достигает миллионов записей в секунду, Kafka позволяет записывать большие объемы
        данных значительно быстрее, чем в традиционных БД или хранилищах ключ-значение
    - Минимум задержки - нужно минимизировать время на передачу сообщений между продюсерами, брокерами и консьюмерами,
        часто это актуально для чатов и интерактивных веб-сайтов, где ожидается мгновенный отклик
    - Высокая надежность - нобходимо гарантировать, что зафиксированные сообщения не будут потеряны. Например, важно
        для конвейеров микросервисов с потоковой передачей событий, где Kafka - это хранилище событий. Еще это важно
        при интеграции между источниками потоковой передачи и постоянными хранилищами (например, AWS S3)
    - Высокая доступность - нужно минимизировать время простоя в случае сбоев

- Оптимизация пропускной способности - сообщения в разных партициях одного топика записываются и читаются одновременно,
    так реализуется параллельная обработка и распределяется нагрузка. Если увеличить количество партиций, обычно расчет
    пропускная способность и снижается риск появления "горячих точек". Но если слишком много партиций, могут быть
    негативные последствия

- Для повышения параллелизма необходимо расчитать нужное количество партиций для каждого топика Kafka. Для этого нужно
    на основе желаемой пропускной способности оценить количество консьюмеров и продюсеров на партицию. Например, если
    цель - скорость чтения 1 ГБ/с, а консьюмер может обрабатывать только 50 МБ/с, то нужно минимум 20 партиций и 20
    консьюмеров в группе. Аналогично для продюсеров, если один продюсер может писать со скоростью 100 МБ/с, то нужно 10
    партиций

- Простая формула расчета количества партиций:
    - примерное_количество_партиций = max(количество_косньюмеров, количество_продюсеров)

- Количество продюсеров/консьюмеров расчитывается на основе общей ожидаемой пропускной способности:
    - колчество_продюсеров = max_speed_per_partition / all_system_speed
    - количество_косньюмеров = max_speed_per_partition / all_system_speed
        - max_speed_per_partition - максимальная пропускная способность одного продюсера/консьюмера для одной партиции
        - all_system_speed - общая ожидаемая пропускная способность

- По формулам выше можно получить хорошую отправную точку, тоесть приблизительно расчитать нужное количество партиций
    на старте. Помимо этого нужно учитывать такие моменты:
    - Перераспределять партиции затратно, поэтому лучше взять с запасом
    - Изменить количество партиций, основанных на ключах, сложно, потребуется ручное копирование
    - Уменьшить количество партиций нельзя. Если это нужно сделать, надо создать новый топик и скопировать в него
        данные
    - При использовании кластера с ZooKeeper метаданные партиций хранятся на нем в виде znodes. Большое количество
        партиций влияет на ZooKeeper и ресурсы клиентов
    - Чем больше партиций, тем больше открытых файловых дескрипторов. Следует установить максимум их количества. При
        этом важно иметь определённый минимум файловых дескрипторов, чтобы брокер эффективно отслеживал файлы сегментов
        журналов. Расчитать разумное количество дескрипторов поможет формула:
        - количество_дескрипторов = количество_партиций * (размер_партиции/размер_сегмента)
        - + брокеру нужны дополнительные дескрипторы для связи через сетевые сокеты с внешними сторонами (клиентами,
            другими брокерами, ZooKeeper, Kerberos)

- Дополнительные аспекты при оптимизации пропускной способности:
    - Размер пакета - ключевой аспект, использование пакетной обработки продюсерами. Продюсеры группируют сообщения в
        пакет => снижается количество запросов к брокерам и нагрузка на CPU. Эффективно использовать пакетную обработке
        помогают параметры:
        - batch.size - максимальный размер пакета сообщений
        - linger.ms - время ожидания продюсером перед отправкой пакета, позволяет пакету наполниться сообщениями и
            достичь заданного размера
        Увеличивая оба параметра увеличивается размер пакета => оптимизируем передачу, использование сетевых ресурсов
        НО необходимо следить за задержкой, тк увеличивается время отклика, важно не переборщить
    - Сжатие - позволяет передавать больше данных в меньшем объёме. Параметр отвечающий за сжатие "compression.type",
        его значения: lz4, snappy, zstd, gzip. Обычно рекомендуется lz4 и не рекомендуется gzip, тк он может сильно
        повлиять на CPU. Сжатие применяется к пакетам данных и его эффективность зависит от заполненности пакетов,
        заполненные пакеты обеспечивают лучший коэффициент сжатия. Если кодек сжатия топика совподает с кодеком
        продюсера, то брокер записывает сжатый пакет непосредственно в лог => не тратится время на повторное сжатие и
        производительность растёт
    - Время ожидания ответов - при отправке сообщение оно отправляется в лидера партиции, продюсер ожидает подтверждение
        фиксации от брокера, прежде чем отправить следующее. Встроенная автоматическая проверка гарантирует, что
        консьюмеры не могут читать незафиксированные сообщения. Время ответа брокера влияет на пропускную способность
        продюсера, оптимизировать процесс помогает параметр "acks" - количество подтверждений от реплик партиций, при
        "acks=1" брокер записывает сообщение в локальный журнал и отправляет подтверждение продюсеру, но
        отказоустойчивость падает, тк в случае падения мастера до синхронизации сообщения с репликами сообщение
        потеряется
    - Лимиты памяти - продюсеры автоматически выделяют память для хранения неотправленных сообщений, при достижении
        лимита продюсер блокирует дополнительные отправки до освобождения памяти и истечении времени, заданного
        параметром "max.blocks.ms". Объём памяти задает параметр "buffer.memory". При большом количестве партиций
        следует увеличить "buffer.memory", учитывая размер сообщений и время ожидания, так поддержится конвейерная
        обработка и оптимизируется пропускная способность брокеров
    - Объём данные при каждой выборке - чем больше этот объём, тем меньше запросов к брокеру => снижается нагрузка на
        CPU, повышается пропускная способность, НО увеличивается задержка, тк брокер ждёт пока в запросе не наберется
        нужное количество данных или не истечет заданное время, параметр за время - "fetch.max.wait.ms", параметр за
        объём "fetch.min.bytes"
    - Другие параметры - рекомендуется использовать группы консьюмеров с несколькими экземплярами, чтобы распараллелить
        обработку и сбалансировать нагрузку между партициями. Максимамальное количество консьюмеро в группе
        ограничивается количеством партиций в топике Так же важно настроить JVM для минимизации паузы во время сборки
        мусора (GC), тк длительные паузы понижают пропускную способность и могут привест к сбоям брокера

- Обработка больших сообщений - можно настроить Kafka на работу с большими сообщениями, изменяя параметры конфигурации
    брокера и консьюмера, касающиеся размера сообщений. Однако перед этим стоит рассмотреть другие варианты:
    - Сжать сообщения - например, исходные текстовые сообщения (например, XML) хорошо сжимаются ("compression.type")
    - Использовать общее хранилище - например, NAS, HDFS, S3. В большинстве случаем намного эффективнее размещать файлы
        в общем хранилище, а через Kafka отправлять сообщения с указанием места расположения файлоа
    - Разделение сообщений - можно разделить сообщения на сегменты, например размером в 1 KB, с помощью продюсера, потом
        отправить эти сегменты послодовательно, далее получить в такой же последовательности на консьюмере и
        восстановить исходное сообщение. Чтобы сегменты отправлялись в одну и ту же партицию можно использовать ключи

- Если варианты выше не подходят, то можно настроить следующие параметры:
    - Для продюсера
        - message.max.bytes - максимальный размер сообщения принимаемый брокером (1 000 000)
        - log.segment.bytes - размер файла данных Kafka, должен быть больше любого сообщения (1 073 741 824)
        - replica.fetch.max.bytes - максимальный размер сообщения, который брокер может реплицировать, значение должно
            превышать размер самого большого сообщения, иначе не все сообщения будут реплицироваться (1 048 576)
    - Для консьюмера
        - max.partition.fetch.bytes - максимальное количество данных на партицию, которое брокер вернёт (1 048 576)
        - fetch.max.bytes - максимальное количество данных, которое сервер может вернуть для выборки (1 073 741 824)

- Консьюмер моджет обработать партию сообщений, размер которой больше чем max.partition.fetch.bytes или fetch.max.bytes,
    но такая партия может отправиться отдельно, что снизит производительность

- Оценка размера кластера - наиболее точный способ это использование "kafka-producer-perf-test" и
    "kafka-consumer-pref-test" на целевом оборудовании для симуляции нагрузки

- Без симуляции оценить размер поможет формула
    объём_диска = ожидаемая_скорость_поступления_данных * требуемый_период_хранения

    - Пусть в системе
        - W - скорость записи данных в MB/с
        - R - фактор репликации
        - C - количество групп консьюмеров (число читателей для каждой записи)

        - Объём записи - каждая реплика записывает каждое сообщение => объём_записи = W * R
        - Объём чтения - данные читаются консьюмерами и репликами в процессе репликации. Каждый консьюмер читает каждую
            запись => объем_чтения_консьюмеров = C * W. Каждая реплика, кроме главной, тоже читает каждую запись =>
            объём_чтения_реплик = (R - 1) * W. А общий_объём_чтения = (R + C - 1) * W
        - Эффект кэширования - чтение может кэшироваться, в этом случае ввод-вывод на диске не происходит. Пусть кластер
            имеет M MB памяти, тогда скорость записи W MB/s позволяет кэшировать записи на время M / (W * R) секунд.
            Напрмер 32 ГБ свободной памяти позволяет хранить в кеше прмерно последние 10 минут данных при скорости
            записи 50 MB/s
        - Эффект выхода из кэша - читатели могут выйти из кэша по разным причинам, например из-за медленного консьюмера
            или сбоя сервера, который восстанавливается и нуждается в синхронизации. Смоделируем! Пусть количество
            отстающих от кэша читателей равно L. Пессимистичное предположение - все консьюмеры отстали, тогда
            L = R + C - 1. Более реалистичный пример - в один момент времени отстаёт не больше 2 консьюмеров

    - Тогда получаем такие требования к вводу-выводу кластера
        - Пропускная способность диска (чтение и запись) - W * R + L * W
        - Сетевая пропускная способность чтения (R + C - 1) * W
        - Сетевая пропускная сопосбность записи W * R

- Определение количества серверов
    - Один сервер обеспечивает определенную пропускную способность диска и сети. Например, сетевой адаптер 1 Gb/s с
        полудуплексным режимом обеспечит 125 MB/s на чтение и 125 MB/s на запись, а 6 дисков SATA 7200 обеспечит
        300 MB/s на чтение и запись. Зная общие требования и возможности одного сервера, посчитаем количество серверов

        - минимальное_число_серверов = общее_количество_нужных_ресурсов / ресурсы_одного_сервера

    - В формуле мы предполагаем, что машина работает на максимальной мощности, нет накладных расходов на протоколы, а
        нагрузка и данные распределены равномерно по серверам. Такого не бывает, чтобы учесть все накладные расходы,
        рекомендуется иметь как минимум в 2 раза больше этой идеально можности. Так обеспечится достаточная ёмкость.
        Получаем
            - достаточное_число_серверов = 2 * (общее_количество_нужных_ресурсов / ресурсы_одного_сервера)

- Общие рекомендации по настройки продюсеров
    - batch.size - по-умолчанию 16384 байт, обычно сразу увеличивают до 100 - 200 КБ
    - linger.ms - по-умолчанию 0, обычно сразу поднимают до 0-100 мс
    - compression.type - по-умолчанию none, обычно ставят lz4
    - acks - по-умолчанию 1, обычно так и оставляют
    - buffer.memory - по-умолчанию 33554432 байт, обычно увеличивают

Для консьюмера
    - fetch.min.bytes - по-умолчанию 1 байт, обычно сразу увеличивают примерно до 100 КБ, для оптимизации объема данных,
        получаемых за одну выборку

- Оптимизация задержки - значения по-умолчанию для многих параметров уже оптимальны для минимизации задержки.

- Чем больше партиций на одном брокере, тем больше пропускная способность, но и тем больше задержка => стоит либо
    увеличить количество брокеров - уменьшая количество партиций на каждый брокер, либо установить общее ограничение на
    количество партиций в кластере

- Что еще поможет оптимизировать задержку:
    - Время ожидания продюсером перед отправкой (linger.ms)
    - Сжатие - отключение экономит такты CPU, но увеличивает нагрузку на сеть. Чаще всего сжатие уменьшает задержку
    - Количество подтверждений - acks = 0 совсем уберет задержку на подтверждение, но риск потери сообщений высокий
    - Параметры fetch.min.bytes, fetch.max.wait.ms - регулируют задержку для консьюмеров. По-умолчанию
        fetch.min.bytes = 1 => запросы выполняются при наличии хотя бы 1 байта данных или по истечении
        fetch.max.wait.ms. Совместная настройка этих параметров оптимизирует размер выборки и время ожидания.

- При работе приложения в режиме реального времени и большом количестве партиций можно увеличить параллелизм
    ввода/вывода на брокере, увеличив кол-во потоков (fetcher) с помощью num.replica.fetchers (по-умолчанию 1).
    Эффективное значение подбирается в процессе тестирования. Например, его стоит увеличить если брокеры не успевают за
    лидером

- Если надо быстро искать в больших таблицах с низкой задержкой рекомендуется использовать локальную потоковую
    обработку. Например, Kafka Connect для локального доступа к удалённым БД, ksqlDB для эффективных локальных
    объединений таблиц и потоков. Эти подходы помогают избегать сетевых запросов для каждой записи

- Параметры для оптимизации задержки
    - На продюсере
        - linger.ms = 0 - время ожидания перед отправкой пакета сообщения
        - compression.type=none - уменьшает задержку со стороны CPU, но увеличивает на стороне сети
        - acks=1/0 - убираем задержку на подтверждения от реплик
    - На консьюмере
        - fetch.min.bytes = 1 - минимальный объем данных для выборки
    - На брокере
        - num.replica.fetchers - если последующие брокеры не успевают за лидером, стоит увеличить кол-во потоков,
            снижает задержку при репликации

- Надёжность в Kafka это снижение вероятности потери сообщений. Ключевой аспект - фактор репликации

- Для топиков с высокими требованиями надежности рекомендуется replication.factor=3 - позволяет выдержать сбой сразу
    двух брокеров

- Если в кластере включена автоматическая генерация топиков (auto.create.topics.enable=true), то можно установить
    default.replication.factor=3, тогда топики созданные автоматически тоже будут реплицироваться. Для ручной настройки
    фактора репликации и партиций следует отключить автоматическое создание топиков auto.create.topics.enable=false

- Надёжность важна не только для пользовательских, но и для внутренних топиков. Например, топик __consumer_offsets,
    который отслеживает смещения должен иметь параметр offsets.topic.replication.factor

- На стороне продюсера можно установить acks=all, чтобы брокер лидер ждал подтверждение от всех синхронизированных
    реплик, прежде чем будет считать сообщение зафиксированным и ответит продюсеру

- Продюсеры могут настроить параметр retries (по-умолчанию MAX_INT) - что позволит повторно отправлять сообщение в
    случае неудачи

- Продюсеры могут настроить максимальное время ожидания между отправкой сообщения и получением подтверждения от брокера
    с помощью параметра delivery.timeout.ms (по-умолчанию 120 000 мс). При автоматических повторных попытках следует
    учитывать два аспетка:
    - В случае кратковременных сбоев продюсер может отправить дублирующие сообщения
    - Повторные попытки могут происходить одновременно с отправкой новых сообщений. Может выйти так, что повторное, но
        более старое сообщение отправится позже более нового и порядок сообщений нарушится

    Для решения этих проблем можно включить идемпотентность enable.idempotence=true - это позволит брокеру отслеживать
        сообщения с помощью порядковых номеров, что предотвращает дублирование и сохраняет порядок сообщений даже при
        конвейерной обработки. Если гарантии идемотентности не удается выполнить, то продюсер выдаст фатальную ошибку и
        разработчику необходимо обработать ее

    Другой сбособ решить проблему это предусмотреть логику обработки дублей на стороне консьюмера

- Kafka обеспечивает надёждность, реплицируя данные между брокерами

- Каждая партиция имеет список назначенных реплик и список синхронизированных реплик (ISR). Лидеры партиций
    автоматически реплицируют репликам из ISR

- При установке acks=all параметр min.insync.replicas задаёт минимальное количество реплик в ISR - если это количество
    не достигнуто, то продюсер выдаст исключение

- Типичный сценарий включает создание топика с replication.factor=3, min.insync.replicas=2 и acks=all - гарантирует что
    продюсер выдаст исключение если большинство реплик не получает запись

- При сбоях брокеров Kafka автоматически обнаруживает сбой и выбирает новых лидеров партиций и действующих реплик

- Брокеры в списке ISR содержат самые свежие сообщения и новый лидер продолжит работу с того места где остановился
    предыдущий, копируя сообщения тем репликам, которые еще не синхронизированы

- Параметр unclean.leader.election.enable определяет могут-ли не синхронизированные реплики с лидером партиции стать
    новым лидером. Для повышения надёжности эту функцию отключают, что предотвращает возможность потери зафиксированных,
    но не сихронизированных сообщений. Однако это может увеличить время простоя, пока другие реплики не синхронизируются

- Хотя обычно не требуется настраивать выгрузку данных, но для критически важных топиков с низкой пропускной
    способностью может потребоваться более частая выгрузка на диск. Для этого можно настроить параметры
    log.flush.interval.ms или log.flush.interval.messages. Например, log.flush.interval.messages=1 позволит каждое
    сообщение сохранять синхронно

- Важно предусмотреть отказ консьюмеров. Консьюмеры отслеживают какие сообщения были считаны, поэтому надо правильно
    настроить процесс фиксации

- Если включена автофиксация смещений на консьюмере, то может произойти такая ситуация, когда фиксируется смещение, а
    далее при обработке сообщения происходит ошибка. Для предотвращения таких ситуаций следует отключить автофиксацию с
    помощью параметра enable.auto.commit=false и явно вызывать метод фиксации смещений после успешной обработки
    сообщения

- Для еще больше надёжности используют EOS (Exactly Once Semantics) - позволяет атомарно записывать данные в несколько
    топиков и партиций

- Консьюмеры могут настраивать параметр isolation.level, чтобы определить какие сообщения получать. Например,
    isolation.level=read_committed - для получения только нетранзакционных или зафиксированных транзакционных сообщений,
    исключая сообщения от открытых или прерванных транзакций

- Для реализации семантику "ровно один раз" в шаблоне "consume-process-produce" клиентское приложение устанавливает
    enable.auto.commit=false и использует метод sendOffsetsToTransaction() в интерфейсе KafkaProducer для фиксации
    смещений

- Для стриминговых приложений можно включить "ровно один раз" установив параметр processing.guarantee

- Сводка параметров для оптимизации надёжности
    - Продюсер
        - replication.factor=3
        - acks=all
        - enable.idempotence=true - для обработки дублирования и упорядочивания сообщений
    - Консьюмер
        - enable.auto.commit=false
        - isolation.level=read_committed - при использовании транзакций с гарантией Exactly Once Semantics (EOS)
    - Брокер
        - default.replication.factor=3
        - auto.create.topics.enable=false
        - min.insync.replicas=2
        - unclean.leader.election.enable=false
        - brocker.rack - стойка брокера (по-умолчанию null)
        - log.flush.interval.messages/log.flush.interval.ms - для топиков с очень низкой пропускной способностью

- Для обеспечения высокой доступности нужно настроить систему на быстрое восстановление после сбоев

- Количество партиций - чем больше партиций, тем выше параллелизм, но тем больше время восстановления, тк брокеры и
    консьюмеры не могут отправлять запросы пока происходит выбор нового лидера для каждой партиции

- Количество подтверждающих реплик - при установке acks=all параметр min.insync.replicas определяет минимальное
    количество репли, которые должны подтвердить запись иначе продюсер выдаст исключение => чем больше
    min.insync.replicas, тем больше может быть сбоев при отправке сообщений => доступность снижается. Если установить
    min.insync.replicas в низкое значение, тем больше система выдержит отказов реплик, тем выше доступность

- Выбор лидера - сбои брокеров вызывают выбор новых лидеров партиций, для оптимальной надёжности лидеры избираются из
    ISR, чтобы избежать потери нереплицированных сообщений, но для повышния доступности можно разрешить выбора нового
    лидера среди и сключённых из ISR брокеров - это ускорит выбор лидера => повысит доступность

- Количество потоков для журнального восстановления - при запуске брокера он сканирует свои журналы для синхронизации
    с другими брокерами, что называется журнальным восстановлением. Количество потоков для выполнения этого процесса
    определяется параметром num.recovery.threads.per.data.dir. Брокеры с болшим количеством сегментов журнала могут
    ичпытывать замедление при загрузке, поэтому имеет смысл увеличить этот параметр до количества дисков в RAID-массиве
    для сокращения времени загрузки

- Поведение системы при сбое консьюмера - консьюмеры объединяются в группы для распределения нагрузки, при выходе из
    строя консьюмера Kafka обнаруживает сбой и перераспределяет партиции между оставшимися. Начиная с KIP-62, активность
    консьюмера отслеживается через heartbeats в фоновом потоке. Таймаут для обнаружения неудачных heartbeats задается
    session.timeout.ms - меньший таймаут позволяет быстрее выявлять сбои, сокращая время восстановления. Рекомендуется
    устанавливать его как можно ниже для обнаружения жестких сбоев, избегая при этом частых мягких. Мягкие сбои чаще
    всего присходят при длительной оработки сообщений или долгой сборке мусора. Для решения проблем с долгой обработкой
    можно увеличить max.poll.interval.ms - консьюмер будет дольше простаивать перед получением новых сообщений или
    уменьшить max.poll.recors - чтобы получать за раз меньше сообщений

- Сводка параметров для высокой доступности
    - unclean.leader.election.enable=true - можно переопределить для каждого топика
    - min.insymc.replicas=1 - можно переопределить для каждого топика
    - num.recovery.threads.per.data.dir - количество логов в log.dirs
